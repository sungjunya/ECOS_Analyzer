# news_scraper.py

import streamlit as st
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import re
import numpy as np 

# âš ï¸ í¬ë¡¤ë§ ì£¼ì˜ ì‚¬í•­: Seleniumì€ requestsë³´ë‹¤ ëŠë¦¬ì§€ë§Œ, 403 ì—ëŸ¬ íšŒí”¼ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.
#    ë¹„ìƒì—…ì  í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì¶©ë¶„í•œ time.sleepì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

@st.cache_data(ttl=600, show_spinner=False)
def scrape_investing_news_titles_selenium(query: str, max_articles: int = 10) -> list:
    """
    í•œêµ­ Investing.com ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ URLì—ì„œ í¬ë¡¤ë§í•œ í›„, 
    ë‹¤ì¤‘ í‚¤ì›Œë“œ(query)ë¥¼ ì´ìš©í•´ í•„í„°ë§í•©ë‹ˆë‹¤. (queryëŠ” 'skí•˜ì´ë‹‰ìŠ¤ sk hynix sk' í˜•íƒœ)
    """
    
    # í•œêµ­ Investing.comì˜ ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ URL ê³ ì •
    base_url = "https://kr.investing.com/news/stock-market-news" 
    target_url = base_url

    news_list = []
    
    # ğŸš¨ ì „ë‹¬ë°›ì€ ë‹¤ì¤‘ í‚¤ì›Œë“œë¥¼ ë¶„ë¦¬ (ì˜ˆ: ['skí•˜ì´ë‹‰ìŠ¤', 'sk', 'hynix'])
    search_keywords = query.lower().split() 
    
    # --- Selenium ì„¤ì • ---
    options = Options()
    options.add_argument("--headless")              
    options.add_argument("--no-sandbox")            
    options.add_argument("--disable-dev-shm-usage") 
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # ì¿¼ë¦¬ë¥¼ í•œêµ­ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        with st.spinner(f"[{query.upper()}] ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ ë¸Œë¼ìš°ì €ë¡œ ë¡œë”© ì¤‘ (5ì´ˆ ëŒ€ê¸°)..."):
            driver.get(target_url) 
            # 403 ì—ëŸ¬ íšŒí”¼ë¥¼ ìœ„í•´ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
            time.sleep(5) 
            soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # --- ë°ì´í„° ì¶”ì¶œ ë° ë‹¤ì¤‘ í•„í„°ë§ ë¡œì§ ---
        # ì œëª© ë§í¬ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œë“¤ì„ ì„ íƒ
        news_containers = soup.select('article a[title]')
        
        for container in news_containers:
            title = container.get('title', '').strip()
            link = container.get('href')
            
            title_lower = title.lower()
            
            # ğŸš¨ [ìˆ˜ì •] ë‹¤ì¤‘ í•„í„°ë§ ë¡œì§ ğŸš¨
            is_relevant = False
            for keyword in search_keywords:
                if keyword in title_lower:
                    is_relevant = True
                    break

            if link and title and is_relevant:
                # kr.investing.com ë„ë©”ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë§í¬ êµ¬ì„±
                full_link = f"https://kr.investing.com{link}" if link.startswith('/') else link
                news_list.append({"title": title, "link": full_link})
            
            if len(news_list) >= max_articles:
                break
                
        driver.quit()
        return news_list

    except Exception as e:
        if driver:
             try: driver.quit()
             except: pass
        st.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ (Selenium) ì‹¤íŒ¨: {e}")
        st.error("Selenium ì„¤ì • ë° ë“œë¼ì´ë²„ ì˜¤ë¥˜ ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return []