# news_scraper.py

import streamlit as st
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import re
import numpy as np 
# ğŸš¨ [ì¶”ê°€] URL ì¸ì½”ë”©ì„ ìœ„í•´ urllib.parse ì„í¬íŠ¸
from urllib.parse import quote 

# âš ï¸ í¬ë¡¤ë§ ì£¼ì˜ ì‚¬í•­: Seleniumì€ requestsë³´ë‹¤ ëŠë¦¬ì§€ë§Œ, 403 ì—ëŸ¬ íšŒí”¼ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.
#    ë¹„ìƒì—…ì  í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì¶©ë¶„í•œ time.sleepì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

@st.cache_data(ttl=600, show_spinner=False)
def scrape_investing_news_titles_selenium(query: str, max_articles: int = 10) -> list:
    """
    í•œêµ­ Investing.comì˜ ì¢…ëª© ê²€ìƒ‰ ë‰´ìŠ¤ ê²°ê³¼ í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    (ì˜ˆ: https://kr.investing.com/search/?q=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&tab=news)
    """
    
    # ğŸš¨ [ìˆ˜ì •] ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ URL ì‚¬ìš©
    encoded_query = quote(query) # í•œêµ­ì–´ ì¿¼ë¦¬ ì¸ì½”ë”©
    base_url = f"https://kr.investing.com/search/?q={encoded_query}&tab=news" 
    target_url = base_url

    news_list = []
    
    # ğŸš¨ [ì‚­ì œ] ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œëŠ” ë³„ë„ì˜ í‚¤ì›Œë“œ í•„í„°ë§ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    #    (ê²€ìƒ‰ ê²°ê³¼ ìì²´ê°€ ì´ë¯¸ í•„í„°ë§ëœ ê²ƒì´ë¯€ë¡œ)
    
    # --- Selenium ì„¤ì • ---
    options = Options()
    options.add_argument("--headless")              
    options.add_argument("--no-sandbox")            
    options.add_argument("--disable-dev-shm-usage") 
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        with st.spinner(f"[{query.upper()}] ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ë¸Œë¼ìš°ì €ë¡œ ë¡œë”© ì¤‘ (5ì´ˆ ëŒ€ê¸°)..."):
            driver.get(target_url) 
            # í˜ì´ì§€ ë¡œë”© ë° ë™ì  ì½˜í…ì¸  ìƒì„±ì„ ìœ„í•´ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
            time.sleep(5) 
            soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # --- ë°ì´í„° ì¶”ì¶œ ë¡œì§ ---
        # ğŸš¨ [ìˆ˜ì •] ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ì œëª©/ë§í¬ CSS Selector
        # Investing.com ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ íƒ­ì˜ ë§í¬ ì»¨í…Œì´ë„ˆ
        news_containers = soup.select('div.search-result-items article a')
        
        for container in news_containers:
            # ì œëª©ì€ a íƒœê·¸ì˜ í…ìŠ¤íŠ¸
            title = container.get_text(strip=True)
            link = container.get('href')
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì´ë¯€ë¡œ ë³„ë„ í‚¤ì›Œë“œ í•„í„°ë§ ë¡œì§ì€ ì‚­ì œ (ì„±ëŠ¥ ê°œì„ )
            
            if link and title:
                # kr.investing.com ë„ë©”ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë§í¬ êµ¬ì„±
                full_link = f"https://kr.investing.com{link}" if link.startswith('/') else link
                news_list.append({"title": title, "link": full_link})
            
            if len(news_list) >= max_articles:
                break
                
        driver.quit()
        return news_list

    except Exception as e:
        if driver:
             try: driver.quit()
             except: pass
        st.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ (Selenium) ì‹¤íŒ¨: {e}")
        st.error("Selenium ì„¤ì • ë° ë“œë¼ì´ë²„ ì˜¤ë¥˜ ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return []