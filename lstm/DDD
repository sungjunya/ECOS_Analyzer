# news_scraper.py íŒŒì¼ ì „ì²´ë¥¼ ì´ ì½”ë“œë¡œ ëŒ€ì²´í•˜ì„¸ìš”.

import streamlit as st
import time
import re
import requests
import certifi
from bs4 import BeautifulSoup

@st.cache_data(ttl=600, show_spinner=False)
def scrape_investing_news_titles_selenium(query: str, max_articles: int = 10) -> list:
    """
    Google News ê²€ìƒ‰ì„ í†µí•´ ë‹¤ì¤‘ í‚¤ì›Œë“œ(query)ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    í•¨ìˆ˜ ì´ë¦„ì€ app.pyì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ì´ë¦„ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    (queryëŠ” 'ì‚¼ì„±ì „ì samsung electronics' í˜•íƒœ)
    """
    
    # ğŸš¨ Google News ê²€ìƒ‰ URL êµ¬ì¡° ğŸš¨
    base_url = "https://news.google.com/search" 
    search_text = query.replace(" ", "+") 

    params = {
        'q': search_text,
        'hl': 'ko',      # Host Language (í•œêµ­ì–´)
        'gl': 'KR',      # Geographic Location (ëŒ€í•œë¯¼êµ­)
        'ceid': 'KR:ko'  # Content Enrollment ID
    }

    headers = {
        # í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ í”íˆ ì‚¬ìš©ë˜ëŠ” User-Agent ì‚¬ìš©
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    
    news_list = []
    
    # ğŸš¨ ì•ˆì •ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•´ 1ì´ˆ ëŒ€ê¸° í›„ ì‹œì‘
    time.sleep(1) 

    try:
        with st.spinner(f"[Google News] '{query.upper()}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰ ì¤‘... (ì„ íƒì ê°•í™” ë²„ì „)"):
            response = requests.get(base_url, headers=headers, params=params, 
                                    timeout=15, verify=certifi.where())
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # --- ë°ì´í„° ì¶”ì¶œ ë¡œì§ (ì„ íƒì ê°•í™”) ---
            # 1. Google Newsì˜ ë©”ì¸ ê¸°ì‚¬ ëª©ë¡ í•­ëª©ì„ ì„ íƒ: 
            #    ì£¼ë¡œ <article> íƒœê·¸ ì•„ë˜ì— ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
            article_items = soup.select('article')
            
            count = 0
            for item in article_items:
                if count >= max_articles:
                    break
                
                # 2. article íƒœê·¸ ì•ˆì—ì„œ ì œëª© ì—­í• ì„ í•˜ëŠ” <a> íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                #    ì œëª©ì€ ì¼ë°˜ì ìœ¼ë¡œ h3 íƒœê·¸ ì•ˆì— ìˆê±°ë‚˜, í° ê¸€ê¼´ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
                #    a íƒœê·¸ ì¤‘ hrefê°€ ./articles/ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤.
                title_tag = item.select_one('a[href^="./articles/"]')
                
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    relative_link = title_tag.get('href')
                    
                    if relative_link and title:
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (https://news.google.com/articles/...)
                        full_link = f"https://news.google.com{relative_link[1:]}"
                        
                        news_list.append({"title": title, "link": full_link})
                        count += 1
            
        if not news_list:
             # ì¬ê²€ì¦ì„ ìœ„í•´, í˜¹ì‹œ ë‹¤ë¥¸ êµ¬ì¡°ë¡œ ë‰´ìŠ¤ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°±ì—… ë¡œì§ ì¶”ê°€
            backup_links = soup.select('a[href^="./articles/"]')
            if backup_links:
                # íŒŒì‹± ë¡œì§ì€ ì‹¤íŒ¨í–ˆì§€ë§Œ ë§í¬ ìì²´ëŠ” ìˆëŠ” ê²½ìš° (ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ)
                 return [{"title": f"ê²½ê³ : ë‰´ìŠ¤ íŒŒì‹±ì€ ì‹¤íŒ¨í–ˆìœ¼ë‚˜, {len(backup_links)}ê°œì˜ ë§í¬ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œì˜ ì„ íƒìë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.", "link": "#"}]
            
            # ìµœì¢…ì ìœ¼ë¡œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ê°€ ë¹ˆ ê²½ìš°
            return [{"title": f"Google News ê²€ìƒ‰ ê²°ê³¼: '{query}'ì™€ ì¼ì¹˜í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° ë¬¸ì œ ë˜ëŠ” ì°¨ë‹¨ ê°€ëŠ¥ì„±)", "link": "#"}]
            
        return news_list

    except requests.exceptions.RequestException as e:
        print(f"Google News í¬ë¡¤ë§ ì‹¤íŒ¨ (Request ì˜¤ë¥˜): {type(e).__name__}, {e}")
        return [{"title": f"Google News í¬ë¡¤ë§ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ì„œë²„ ì˜¤ë¥˜)", "link": "#"}]
    except Exception as e:
        print(f"ë‰´ìŠ¤ í¬ë¡¤ë§ (Google News) ì‹¤íŒ¨: {type(e).__name__}, {e}")
        return [{"title": f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨ (ê¸°íƒ€ ì˜¤ë¥˜)", "link": "#"}]