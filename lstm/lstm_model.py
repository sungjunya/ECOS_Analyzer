# lstm_model.py (ìˆ˜ì •ëœ ìµœì¢… ì½”ë“œ)
import streamlit as st
import numpy as np
import pandas as pd 
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input 
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import os
import tensorflow as tf 
import numpy as np 
from sklearn.metrics import mean_squared_error 
from joblib import dump, load # joblib.load, joblib.dump ëŒ€ì‹  ëª…ì‹œì ìœ¼ë¡œ ìž„í¬íŠ¸

MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

@st.cache_resource
def _train_and_evaluate_model(df, symbol, time_steps=60): 
    df = df.copy()
    
    # ... (ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ë¡œì§: ë³€ë™ ì—†ìŒ)
    df['SMA_5'] = df['Close'].rolling(5).mean()
    df['SMA_20'] = df['Close'].rolling(20).mean()
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / loss.replace(0, 1e-6)
    df['RSI'] = 100 - (100 / (1 + rs))
    ema12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = ema12 - ema26
    df['Volume_SMA'] = df['Volume'].rolling(20).mean()
    
    df['BB_Std'] = df['Close'].rolling(window=20).std()
    df['BB_Upper'] = df['SMA_20'] + (df['BB_Std'] * 2)
    df['BB_Lower'] = df['SMA_20'] - (df['BB_Std'] * 2)
    
    df['OBV'] = (df['Close'].diff().apply(np.sign) * df['Volume']).fillna(0).cumsum()
    
    high_14 = df['High'].rolling(window=14).max()
    low_14 = df['Low'].rolling(window=14).min()
    df['Stoch_K'] = 100 * ((df['Close'] - low_14) / (high_14 - low_14).replace(0, 1e-6))
    df['Stoch_D'] = df['Stoch_K'].rolling(window=3).mean()
    
    df['ROC'] = (df['Close'] - df['Close'].shift(9)) / df['Close'].shift(9) * 100
    
    df = df.dropna()
    
    features = ['Close', 'Volume', 'SMA_5', 'SMA_20', 'RSI', 'MACD', 'Volume_SMA', 
                'BB_Upper', 'BB_Lower', 'OBV', 'Stoch_K', 'Stoch_D', 'ROC']
    data = df[features].values
    
    if len(data) < time_steps:
        st.error(f"ì§€í‘œ ìƒì„± í›„ ë°ì´í„° ë¶€ì¡±! {len(data)}ì¼ < {time_steps}ì¼")
        return None, None, None, None, None, None 

    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(data)
    
    dates_for_sequences = df.index[time_steps:] 

    X, y = [], []
    for i in range(time_steps, len(scaled)):
        X.append(scaled[i-time_steps:i])
        y.append(scaled[i, 0]) 
    X, y = np.array(X), np.array(y)

    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    test_dates = dates_for_sequences[train_size:]
    
    model = Sequential([
        Input(shape=(time_steps, len(features))), # Input Layer
        LSTM(100, return_sequences=True), # The Feature Extractor
        LSTM(100), # The Pattern Analyzer
        Dense(50), 
        Dense(1) # Output Layer
    ])
    model.compile(optimizer='adam', loss='mse') #Adaptive Moment Estimation
    
    with st.spinner("LSTM ë‹¤ë³€ëŸ‰ ëª¨ë¸ í•™ìŠµ"):
        model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=0,
                # EarlyStoppingìœ¼ë¡œ 7ë²ˆ í•™ìŠµ ì‹œì—ë„ Lossê°’ ê°œì„ ë˜ì§€ ì•Šì„ ì‹œ ê³¼ì í•©ìœ¼ë¡œ íŒë‹¨ (ë°©ì§€ìš©)
                callbacks=[EarlyStopping(patience=7, restore_best_weights=True, monitor='loss')]) 

    scaled_test_y_pred = model.predict(X_test)
    
    # ----------------------------------------------------------------------------------
    # ðŸš¨ [í•µì‹¬ ìˆ˜ì • ë¶€ë¶„] RMSE/MAE ê³„ì‚°ì„ ìœ„í•´ ì •ê·œí™”ëœ ê°’(y_test, scaled_test_y_pred)ì„ ë°˜í™˜
    # ----------------------------------------------------------------------------------
    
    # 1. ì§€í‘œ ê³„ì‚°ìš©: ì •ê·œí™”ëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš© (app.pyì˜ calculate_metrics í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•  ê°’)
    # y_testëŠ” ì´ë¯¸ ì •ê·œí™”ëœ ìƒíƒœìž…ë‹ˆë‹¤. scaled_test_y_predë„ ë§ˆì°¬ê°€ì§€ìž…ë‹ˆë‹¤.
    test_y_true_scaled = y_test
    test_y_pred_scaled = scaled_test_y_pred.flatten() 

    # 2. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ìž¥ (ë³€ë™ ì—†ìŒ)
    safe_symbol = symbol.replace(".", "_")
    scaler_path = os.path.join(MODEL_DIR, f"scaler_{safe_symbol}_{time_steps}.pkl")
    model_path = os.path.join(MODEL_DIR, f"model_{safe_symbol}_{time_steps}.keras")
    
    joblib.dump(scaler, scaler_path)
    model.save(model_path)
    
    st.success(f"ë‹¤ë³€ëŸ‰ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: `{model_path}`")
    
    # 3. ë°˜í™˜ ê°’ ë³€ê²½: test_y_true, test_y_predë¥¼ scaled ê°’ìœ¼ë¡œ ë³€ê²½
    return scaler, model, df.drop(columns=['BB_Std']), test_y_true_scaled, test_y_pred_scaled, test_dates 

def train_lstm_model(df, symbol, time_steps=60):
    # ðŸš¨ _train_and_evaluate_modelì—ì„œ scaled ê°’ì„ ë°˜í™˜ë°›ìŒ
    scaler, model, processed_df, test_y_true_scaled, test_y_pred_scaled, test_dates = _train_and_evaluate_model(df, symbol, time_steps)
    
    if model:
        st.session_state.model_trained = True
        st.session_state.model_symbol = symbol
        st.session_state.model_time_steps = time_steps
        st.session_state.processed_df = processed_df
        
        st.session_state.test_dates = test_dates
        
        # ðŸš¨ app.pyë¡œ scaled ê°’ì„ ì „ë‹¬í•˜ì—¬, app.pyì—ì„œ scaled ì§€í‘œê°€ ê³„ì‚°ë˜ë„ë¡ í•¨
        return test_y_true_scaled, test_y_pred_scaled
    else:
        return np.array([]), np.array([])